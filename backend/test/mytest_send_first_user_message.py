import streamlit as st
import requests
import time
import json
from datetime import datetime

st.set_page_config(page_title="è‡ªåŠ¨å‘é€é¦–æ¡ç”¨æˆ·ä¿¡æ¯æµ‹è¯•", page_icon="ğŸ§ª", layout="wide")

# APIåŸºç¡€URL
if "api_base_url" not in st.session_state:
    st.session_state.api_base_url = "http://localhost:8000"

# ä¼šè¯ç®¡ç†
if "all_session_ids" not in st.session_state:
    st.session_state.all_session_ids = [f"test_session_{int(time.time())}"]
if "current_session_index" not in st.session_state:
    st.session_state.current_session_index = 0
if "session_models" not in st.session_state:
    st.session_state.session_models = {}  # æ¯ä¸ªä¼šè¯çš„å½“å‰æ¨¡å‹
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}

st.session_state.session_id = st.session_state.all_session_ids[st.session_state.current_session_index]

# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = "deepseek-chat"

# å·¥å…·å‡½æ•°

def api_post(endpoint, data, stream_output=False):
    url = f"{st.session_state.api_base_url}{endpoint}"
    headers = {"Content-Type": "application/json"}
    # åªæœ‰/qa/chatæ‰å…è®¸æµå¼
    if endpoint == "/api/llm/qa/chat" and stream_output:
        response = requests.post(url, json=data, headers=headers, stream=True)
        response.raise_for_status()
        def stream_gen():
            for chunk in response.iter_content(chunk_size=1024):
                if chunk:
                    yield chunk.decode("utf-8", errors="ignore")
        return stream_gen()
    else:
        resp = requests.post(url, json=data, headers=headers)
        resp.raise_for_status()
        return resp.json()

def api_get(endpoint):
    url = f"{st.session_state.api_base_url}{endpoint}"
    resp = requests.get(url)
    resp.raise_for_status()
    return resp.json()

# ä¾§è¾¹æ ï¼šä¼šè¯ç®¡ç†
with st.sidebar:
    st.header("ä¼šè¯ç®¡ç†")
    selected = st.selectbox(
        "é€‰æ‹©ä¼šè¯",
        options=[f"{s} (å½“å‰)" if s == st.session_state.session_id else s for s in st.session_state.all_session_ids],
        index=st.session_state.current_session_index
    )
    actual_selected = selected.replace(" (å½“å‰)", "")
    new_index = st.session_state.all_session_ids.index(actual_selected)
    if new_index != st.session_state.current_session_index:
        st.session_state.current_session_index = new_index
        st.session_state.session_id = st.session_state.all_session_ids[new_index]
        st.rerun()

    if st.button("â• åˆ›å»ºæ–°ä¼šè¯"):
        new_id = f"test_session_{int(time.time())}"
        st.session_state.all_session_ids.append(new_id)
        st.session_state.current_session_index = len(st.session_state.all_session_ids) - 1
        st.session_state.session_id = new_id
        st.session_state.session_models[new_id] = DEFAULT_MODEL
        st.session_state.chat_history[new_id] = []
        st.rerun()

    st.text_input("å½“å‰ä¼šè¯ID", value=st.session_state.session_id, disabled=True)

# ä¸»ç•Œé¢
st.title("ğŸ§ª è‡ªåŠ¨å‘é€é¦–æ¡ç”¨æˆ·ä¿¡æ¯æ¥å£æµ‹è¯•")

# æ¨¡å‹åˆ‡æ¢
st.subheader("å½“å‰æ¨¡å‹")
model_name = st.text_input("æ¨¡å‹åç§°", value=st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL))
if model_name != st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL):
    st.session_state.session_models[st.session_state.session_id] = model_name
    st.success(f"å·²åˆ‡æ¢æ¨¡å‹ä¸º: {model_name}")

# æ—…æ¸¸promptç”Ÿæˆå™¨å’Œè‡ªåŠ¨AIå›å¤
with st.expander("ğŸš— ç”Ÿæˆæ—…æ¸¸ç”¨æˆ·æç¤ºè¯å¹¶è‡ªåŠ¨å‘é€ (æ¨èä½“éªŒ)", expanded=True):
    with st.form("travel_form"):
        destination = st.text_input("ç›®çš„åœ°", "ä¸‰äºš")
        date = st.text_input("æ—¥æœŸ", "2024-08-01")
        budget = st.text_input("é¢„ç®—", "5000å…ƒ")
        preference = st.text_input("ç”¨æˆ·åå¥½", "å–œæ¬¢æµ·è¾¹ã€æ€•æ™’ã€å–œæ¬¢è½»ä¾¿ç©¿æ­")
        submitted = st.form_submit_button("ç”Ÿæˆç”¨æˆ·æç¤ºè¯å¹¶è‡ªåŠ¨å‘é€")
        if submitted:
            req = {"destination": destination, "date": date, "budget": budget, "preference": preference}
            resp = api_post("/api/prompt/build_travel_prompt", req, stream_output=False)
            user_prompt = resp["user_prompt"]
            st.write("ç”Ÿæˆçš„ç”¨æˆ·æç¤ºè¯:")
            st.code(user_prompt)
            send_req = {"session_id": st.session_state.session_id, "user_prompt": user_prompt}
            send_resp = api_post("/api/prompt/send_first_user_message", send_req, stream_output=False)
            # è‡ªåŠ¨æµå¼å‘é€ç»™å¤§æ¨¡å‹å¹¶å±•ç¤ºå›å¤
            chat_data = {
                "user_message": user_prompt,
                "session_id": st.session_state.session_id,
                "model_name": st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL),
                "temperature": 0.7
            }
            st.info(f"å·²è‡ªåŠ¨å‘é€é¦–æ¡ç”¨æˆ·ä¿¡æ¯: {user_prompt}")
            st.subheader("ğŸ¤– AI é¦–æ¬¡å›å¤ (æµå¼)")
            message_placeholder = st.empty()
            full_response = ""
            response_stream = api_post("/api/llm/qa/chat", chat_data, stream_output=True)
            try:
                for chunk in response_stream:
                    lines = chunk.strip().split("\n")
                    for line in lines:
                        if line.startswith("data:"):
                            try:
                                data_str = line[len("data:"):].strip()
                                data_json = json.loads(data_str)
                                if data_json.get("type") == "content":
                                    content = data_json.get("content", "")
                                    full_response += content
                                    message_placeholder.markdown(full_response + "â–Œ")
                            except Exception as e:
                                st.error(f"è§£ææµæ•°æ®å‡ºé”™ï¼š{e}")
                message_placeholder.markdown(full_response)
            except Exception as e:
                st.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # å­˜å…¥æœ¬åœ°å†å²
            hist_key = st.session_state.session_id
            if hist_key not in st.session_state.chat_history:
                st.session_state.chat_history[hist_key] = []
            st.session_state.chat_history[hist_key].append({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "user_message": user_prompt,
                "response": full_response,
                "model_name": st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL)
            })

# èŠå¤©è¾“å…¥
st.subheader("ğŸ’¬ å‘é€ç”¨æˆ·æ¶ˆæ¯ (æµå¼)")
user_message = st.text_area("ç”¨æˆ·æ¶ˆæ¯", height=80, key=f"user_msg_{st.session_state.session_id}")
if st.button("ğŸš€ å‘é€æ¶ˆæ¯", key=f"send_{st.session_state.session_id}"):
    if not user_message.strip():
        st.warning("è¯·è¾“å…¥æ¶ˆæ¯å†…å®¹")
    else:
        chat_data = {
            "user_message": user_message,
            "session_id": st.session_state.session_id,
            "model_name": st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL),
            "temperature": 0.7
        }
        response_stream = api_post("/api/llm/qa/chat", chat_data, stream_output=True)
        st.subheader("ğŸ¤– AI å›å¤ (æµå¼)")
        message_placeholder = st.empty()
        full_response = ""
        try:
            for chunk in response_stream:
                lines = chunk.strip().split("\n")
                for line in lines:
                    if line.startswith("data:"):
                        try:
                            data_str = line[len("data:"):].strip()
                            data_json = json.loads(data_str)
                            if data_json.get("type") == "content":
                                content = data_json.get("content", "")
                                full_response += content
                                message_placeholder.markdown(full_response + "â–Œ")
                        except Exception as e:
                            st.error(f"è§£ææµæ•°æ®å‡ºé”™ï¼š{e}")
            message_placeholder.markdown(full_response)
            # å­˜å…¥æœ¬åœ°å†å²
            hist_key = st.session_state.session_id
            if hist_key not in st.session_state.chat_history:
                st.session_state.chat_history[hist_key] = []
            st.session_state.chat_history[hist_key].append({
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "user_message": user_message,
                "response": full_response,
                "model_name": st.session_state.session_models.get(st.session_state.session_id, DEFAULT_MODEL)
            })
        except Exception as e:
            st.error(f"å¤„ç†æµå¼å“åº”æ—¶å‘ç”Ÿé”™è¯¯: {e}")

# å±•ç¤ºæœ¬åœ°å†å²
hist_key = st.session_state.session_id
if hist_key in st.session_state.chat_history and st.session_state.chat_history[hist_key]:
    st.subheader("ğŸ’­ æœ¬åœ°ä¼šè¯å†å²")
    for chat in reversed(st.session_state.chat_history[hist_key][-5:]):
        with st.expander(f"({chat['timestamp']}) - {chat['user_message'][:30]}..."):
            st.markdown(f"**ğŸ‘¤ ç”¨æˆ·:**\n> {chat['user_message']}")
            st.markdown(f"**ğŸ¤– AI ({chat['model_name']}):**\n> {chat['response']}") 