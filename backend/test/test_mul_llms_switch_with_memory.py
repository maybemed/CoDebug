# @user: maybemed
# @last_update: 2025-07-11 11:25:05 UTC

import time

from backend.core.llm.available_llms import AvailableLLMs
from backend.core.llm.llm_manager import LLMManager
from backend.core.llm import available_llms
from backend.core.prompt_manager import PromptManager


def test_multi_instance_isolation():
    """æµ‹è¯•å¤šå®ä¾‹éš”ç¦»"""
    print("=" * 60)
    print("æµ‹è¯• 1: å¤šå®ä¾‹éš”ç¦»")
    print("=" * 60)

    # åˆ›å»ºåŒä¸€æ¨¡å‹çš„å¤šä¸ªå®ä¾‹
    instance1 = LLMManager.create_llm_instance(
        instance_id="deepseek_instance_1",
        model_name=AvailableLLMs.DeepSeek,
        temperature=0.7
    )

    instance2 = LLMManager.create_llm_instance(
        instance_id="deepseek_instance_2",
        model_name=AvailableLLMs.DeepSeek,
        temperature=0.3  # ä¸åŒçš„æ¸©åº¦
    )

    # åœ¨å®ä¾‹1ä¸­è¿›è¡Œå¯¹è¯
    print("\nğŸ”µ å®ä¾‹1å¯¹è¯:")
    print("ç”¨æˆ·: æˆ‘æ˜¯ Aliceï¼Œå–œæ¬¢ç»˜ç”»")
    response1 = LLMManager.chat_with_instance(
        instance_id="deepseek_instance_1",
        user_message="æˆ‘æ˜¯ Aliceï¼Œå–œæ¬¢ç»˜ç”»",
        session_id="session_001"
    )
    print(f"AI: {response1}")

    # åœ¨å®ä¾‹2ä¸­è¿›è¡Œå¯¹è¯
    print("\nğŸ”´ å®ä¾‹2å¯¹è¯:")
    print("ç”¨æˆ·: æˆ‘æ˜¯ Bobï¼Œå–œæ¬¢ç¼–ç¨‹")
    response2 = LLMManager.chat_with_instance(
        instance_id="deepseek_instance_2",
        user_message="æˆ‘æ˜¯ Bobï¼Œå–œæ¬¢ç¼–ç¨‹",
        session_id="session_001"  # ç›¸åŒçš„session_id
    )
    print(f"AI: {response2}")

    # æµ‹è¯•è®°å¿†éš”ç¦»
    print("\nğŸ” æµ‹è¯•è®°å¿†éš”ç¦»:")
    print("å®ä¾‹1 - ç”¨æˆ·: ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’Œçˆ±å¥½å—ï¼Ÿ")
    response3 = LLMManager.chat_with_instance(
        instance_id="deepseek_instance_1",
        user_message="ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’Œçˆ±å¥½å—ï¼Ÿ",
        session_id="session_001"
    )
    print(f"å®ä¾‹1 AI: {response3}")

    print("\nå®ä¾‹2 - ç”¨æˆ·: ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’Œçˆ±å¥½å—ï¼Ÿ")
    response4 = LLMManager.chat_with_instance(
        instance_id="deepseek_instance_2",
        user_message="ä½ è¿˜è®°å¾—æˆ‘çš„åå­—å’Œçˆ±å¥½å—ï¼Ÿ",
        session_id="session_001"
    )
    print(f"å®ä¾‹2 AI: {response4}")

    print("\nâœ… ä¸¤ä¸ªå®ä¾‹åº”è¯¥åˆ†åˆ«è®°ä½ Alice(ç»˜ç”») å’Œ Bob(ç¼–ç¨‹)")


def test_model_switching():
    """æµ‹è¯•æ¨¡å‹åˆ‡æ¢åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: æ¨¡å‹åˆ‡æ¢åŠŸèƒ½")
    print("=" * 60)

    session_id = "model_switching_session"

    # ä½¿ç”¨ DeepSeek å¼€å§‹å¯¹è¯
    print("\nğŸ¤– ä½¿ç”¨ DeepSeek æ¨¡å‹:")
    print("ç”¨æˆ·: æˆ‘åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œè¯·ç»™æˆ‘ä¸€äº›ç®€çŸ­çš„å»ºè®®")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_model_switching_stream(
            user_message="æˆ‘åœ¨å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Œè¯·ç»™æˆ‘ä¸€äº›ç®€çŸ­çš„å»ºè®®",
            session_id=session_id,
            model_name=AvailableLLMs.DeepSeek
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    # åˆ‡æ¢åˆ° Qwen ç»§ç»­å¯¹è¯
    print("\n\nğŸ§  åˆ‡æ¢åˆ° Qwen æ¨¡å‹:")
    print("ç”¨æˆ·: åˆšæ‰ä½ æåˆ°çš„ç¬¬ä¸€ç‚¹ï¼Œèƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_model_switching_stream(
            user_message="åˆšæ‰ä½ æåˆ°çš„ç¬¬ä¸€ç‚¹ï¼Œèƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹å—ï¼Ÿ",
            session_id=session_id,
            model_name=AvailableLLMs.Qwen  # åˆ‡æ¢æ¨¡å‹
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    # å†åˆ‡æ¢åˆ° Zhipu
    print("\n\nğŸ¯ åˆ‡æ¢åˆ° Zhipu æ¨¡å‹:")
    print("ç”¨æˆ·: åŸºäºæˆ‘ä»¬çš„è®¨è®ºï¼Œæ¨èä¸€ä¸ªå­¦ä¹ è·¯å¾„")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_model_switching_stream(
            user_message="åŸºäºæˆ‘ä»¬çš„è®¨è®ºï¼Œæ¨èä¸€ä¸ªå­¦ä¹ è·¯å¾„",
            session_id=session_id,
            model_name=AvailableLLMs.Zhipu  # å†æ¬¡åˆ‡æ¢
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    print("\n\nâœ… æ¨¡å‹åˆ‡æ¢å®Œæˆï¼Œå¯¹è¯è®°å¿†åº”è¯¥ä¿æŒè¿ç»­")


def test_management_functions():
    """æµ‹è¯•ç®¡ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: ç®¡ç†åŠŸèƒ½")
    print("=" * 60)

    # åˆ—å‡ºæ‰€æœ‰å®ä¾‹
    print("ğŸ“‹ æ‰€æœ‰ LLM å®ä¾‹:")
    instances = LLMManager.list_llm_instances()
    for instance_id, stats in instances.items():
        print(
            f"  {instance_id}: {stats['model_name']} (T={stats['temperature']}, ä¼šè¯æ•°={stats['active_conversations']})")

    # åˆ—å‡ºæ‰€æœ‰å…±äº«å¯¹è¯
    print("\nğŸ“‹ æ‰€æœ‰å…±äº«å¯¹è¯:")
    conversations = LLMManager.list_shared_conversations()
    for session_id, stats in conversations.items():
        print(f"  {session_id}: å½“å‰æ¨¡å‹={stats['current_model']}, æ¶ˆæ¯æ•°={stats['total_messages']}")
        print(f"    æ¨¡å‹ä½¿ç”¨ç»Ÿè®¡: {stats['model_usage']}")

    # æŸ¥çœ‹å¯¹è¯å†å²
    print("\nğŸ“œ æŸ¥çœ‹å…±äº«å¯¹è¯å†å²:")
    history = LLMManager.get_shared_conversation_history("creative_writing")
    print(f"å†å²æ¶ˆæ¯æ•°: {len(history)}")
    for i, msg in enumerate(history[-4:], 1):  # æ˜¾ç¤ºæœ€å4æ¡
        msg_type = type(msg).__name__.replace("Message", "")
        content = msg.content[:50] + "..." if len(msg.content) > 50 else msg.content
        print(f"  {i}. [{msg_type}] {content}")

    print("\nâœ… ç®¡ç†åŠŸèƒ½æµ‹è¯•å®Œæˆ")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¤šå®ä¾‹å’Œæ¨¡å‹åˆ‡æ¢åŠŸèƒ½æµ‹è¯•")
    print(f"å½“å‰æ—¶é—´: 2025-07-11 11:25:05 UTC")
    print(f"å½“å‰ç”¨æˆ·: maybemed")
    print("=" * 60)

    try:
        # test_multi_instance_isolation()
        test_model_switching()
        test_management_functions()

        print("\n" + "ğŸ‰" * 20)
        print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ‰" * 20)

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()