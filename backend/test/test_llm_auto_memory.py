# @user: maybemed
# @last_update: 2025-07-11 09:08:45 UTC

import time
from backend.core.llm.llm_manager import LLMManager
from backend.core.llm.available_llms import AvailableLLMs
from backend.core.prompt_manager import PromptManager

def test_llm_memory_stream():
    """æµ‹è¯•æµå¼ LLM è®°å¿†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 1: æµå¼ LLM è®°å¿†åŠŸèƒ½")
    print("=" * 60)

    session_id = "maybemed_stream_test"

    # ç¬¬ä¸€è½®æµå¼å¯¹è¯
    print("\nğŸ“ ç¬¬ä¸€è½®æµå¼å¯¹è¯:")
    print("ç”¨æˆ·: æˆ‘æ­£åœ¨å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨å¼€å‘")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_memory_stream(
            user_message="æˆ‘æ­£åœ¨å­¦ä¹ å¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨å¼€å‘",
            session_id=session_id,
            model_name=AvailableLLMs.DeepSeek
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    # ç¬¬äºŒè½®æµå¼å¯¹è¯
    print("\n\nğŸ“ ç¬¬äºŒè½®æµå¼å¯¹è¯:")
    print("ç”¨æˆ·: èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_memory_stream(
            user_message="èƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ",
            session_id=session_id,
            model_name=AvailableLLMs.DeepSeek
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    print("\n\nâœ… LLM åº”è¯¥åŸºäºç¬¬ä¸€è½®çš„å­¦ä¹ å†…å®¹ç»™å‡ºç›¸å…³å»ºè®®")


def test_llm_memory_management():
    """æµ‹è¯• LLM è®°å¿†ç®¡ç†åŠŸèƒ½"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: LLM è®°å¿†ç®¡ç†åŠŸèƒ½")
    print("=" * 60)

    session_id = "maybemed_memory_mgmt"

    # æ·»åŠ ä¸€äº›å¯¹è¯
    print("ğŸ“ æ·»åŠ å¯¹è¯åˆ°è®°å¿†...")
    LLMManager.chat_with_memory(
        user_message="æˆ‘çš„å…´è¶£æ˜¯æœºå™¨å­¦ä¹ ",
        session_id=session_id,
        model_name=AvailableLLMs.DeepSeek
    )

    LLMManager.chat_with_memory(
        user_message="æˆ‘ç›®å‰åœ¨å­¦ä¹  transformer æ¶æ„",
        session_id=session_id,
        model_name=AvailableLLMs.DeepSeek
    )

    # æŸ¥çœ‹å¯¹è¯å†å²
    print("\nğŸ“‹ æŸ¥çœ‹å¯¹è¯å†å²:")
    history = LLMManager.get_conversation_history(session_id)
    print(f"å¯¹è¯å†å²ä¸­æœ‰ {len(history)} æ¡æ¶ˆæ¯:")
    for i, msg in enumerate(history, 1):
        msg_type = type(msg).__name__.replace("Message", "")
        content = msg.content[:60] + "..." if len(msg.content) > 60 else msg.content
        print(f"  {i}. [{msg_type}] {content}")

    # æµ‹è¯•è®°å¿†æ•ˆæœ
    print("\nğŸ§  æµ‹è¯•è®°å¿†æ•ˆæœ:")
    print("ç”¨æˆ·: æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰èŠçš„å†…å®¹")
    print("AI: ", end="", flush=True)

    for chunk in LLMManager.chat_with_memory_stream(
            user_message="æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬åˆšæ‰èŠçš„å†…å®¹",
            session_id=session_id,
            model_name=AvailableLLMs.DeepSeek
    ):
        print(chunk, end="", flush=True)
        time.sleep(0.02)

    # æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
    print("\n\nğŸ“Š å¯¹è¯ç»Ÿè®¡ä¿¡æ¯:")
    stats = LLMManager.get_conversation_stats(session_id)
    print(f"  ä¼šè¯ID: {stats.get('session_id')}")
    print(f"  æ€»æ¶ˆæ¯æ•°: {stats.get('total_messages')}")
    print(f"  æ¶ˆæ¯ç±»å‹: {stats.get('message_types')}")
    print(f"  æ€»å­—ç¬¦æ•°: {stats.get('total_characters')}")
    print(f"  ä¼°ç®—tokenæ•°: {stats.get('estimated_tokens')}")

    # æ¸…é™¤è®°å¿†
    print("\nğŸ—‘ï¸ æ¸…é™¤è®°å¿†...")
    LLMManager.clear_conversation_history(session_id)

    # éªŒè¯è®°å¿†å·²æ¸…é™¤
    print("éªŒè¯è®°å¿†æ¸…é™¤æ•ˆæœ:")
    print("ç”¨æˆ·: æˆ‘ä»¬åˆšæ‰èŠäº†ä»€ä¹ˆï¼Ÿ")
    response = LLMManager.chat_with_memory(
        user_message="æˆ‘ä»¬åˆšæ‰èŠäº†ä»€ä¹ˆï¼Ÿ",
        session_id=session_id,
        model_name=AvailableLLMs.DeepSeek
    )
    print(f"AI: {response}")
    print("âœ… è®°å¿†å·²æ¸…é™¤ï¼ŒAI ä¸å†è®°å¾—ä¹‹å‰çš„å¯¹è¯")


def test_llm_multiple_sessions():
    """æµ‹è¯•å¤šä¼šè¯éš”ç¦»"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 5: LLM å¤šä¼šè¯éš”ç¦»")
    print("=" * 60)

    # ä¼šè¯ A
    session_a = "maybemed_session_A"
    print("ğŸ…°ï¸ ä¼šè¯ A:")
    print("ç”¨æˆ·: æˆ‘æ˜¯å‰ç«¯å¼€å‘è€…ï¼Œä¸“æ³¨ React å¼€å‘")
    LLMManager.chat_with_memory(
        user_message="æˆ‘æ˜¯å‰ç«¯å¼€å‘è€…ï¼Œä¸“æ³¨ React å¼€å‘",
        session_id=session_a,
        model_name=AvailableLLMs.DeepSeek
    )
    print("âœ… ä¼šè¯ A è®°å½•å®Œæˆ")

    # ä¼šè¯ B
    session_b = "maybemed_session_B"
    print("\nğŸ…±ï¸ ä¼šè¯ B:")
    print("ç”¨æˆ·: æˆ‘æ˜¯åç«¯å·¥ç¨‹å¸ˆï¼Œä¸»è¦ç”¨ Python Django")
    LLMManager.chat_with_memory(
        user_message="æˆ‘æ˜¯åç«¯å·¥ç¨‹å¸ˆï¼Œä¸»è¦ç”¨ Python Django",
        session_id=session_b,
        model_name=AvailableLLMs.DeepSeek
    )
    print("âœ… ä¼šè¯ B è®°å½•å®Œæˆ")

    # åœ¨ä¼šè¯ A ä¸­æµ‹è¯•è®°å¿†
    print("\nğŸ” åœ¨ä¼šè¯ A ä¸­æµ‹è¯•:")
    print("ç”¨æˆ·: ä½ çŸ¥é“æˆ‘çš„æŠ€æœ¯æ ˆæ˜¯ä»€ä¹ˆå—ï¼Ÿ")
    response_a = LLMManager.chat_with_memory(
        user_message="ä½ çŸ¥é“æˆ‘çš„æŠ€æœ¯æ ˆæ˜¯ä»€ä¹ˆå—ï¼Ÿ",
        session_id=session_a,
        model_name=AvailableLLMs.DeepSeek
    )
    print(f"AI: {response_a}")
    print("âœ… ä¼šè¯ A åº”è¯¥å›ç­” React å‰ç«¯å¼€å‘")

    # åœ¨ä¼šè¯ B ä¸­æµ‹è¯•è®°å¿†
    print("\nğŸ” åœ¨ä¼šè¯ B ä¸­æµ‹è¯•:")
    print("ç”¨æˆ·: ä½ çŸ¥é“æˆ‘çš„æŠ€æœ¯æ ˆæ˜¯ä»€ä¹ˆå—ï¼Ÿ")
    response_b = LLMManager.chat_with_memory(
        user_message="ä½ çŸ¥é“æˆ‘çš„æŠ€æœ¯æ ˆæ˜¯ä»€ä¹ˆå—ï¼Ÿ",
        session_id=session_b,
        model_name=AvailableLLMs.DeepSeek
    )
    print(f"AI: {response_b}")
    print("âœ… ä¼šè¯ B åº”è¯¥å›ç­” Python Django åç«¯å¼€å‘")

    # æ˜¾ç¤ºæ‰€æœ‰æ´»è·ƒä¼šè¯
    print("\nğŸ“‹ æ‰€æœ‰æ´»è·ƒ LLM ä¼šè¯:")
    active_conversations = LLMManager.list_active_conversations()
    print(f"æ´»è·ƒä¼šè¯: {active_conversations}")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§  LLM è®°å¿†åŠŸèƒ½å®Œæ•´æµ‹è¯•")
    print(f"å½“å‰æ—¶é—´: 2025-07-11 09:08:45 UTC")
    print(f"å½“å‰ç”¨æˆ·: maybemed")
    print("=" * 60)

    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_llm_memory_stream()
        print("="*46)
        print("="*46)
        test_llm_memory_management()
        print("="*46)
        print("="*46)
        test_llm_multiple_sessions()

        print("\n" + "ğŸ‰" * 20)
        print("æ‰€æœ‰ LLM è®°å¿†æµ‹è¯•å®Œæˆï¼")
        print("ğŸ‰" * 20)

        # æœ€ç»ˆç»Ÿè®¡
        print("\nğŸ“Š æœ€ç»ˆ LLM å¯¹è¯ç»Ÿè®¡:")
        all_stats = LLMManager.get_conversation_stats()
        print(f"æ€»ä¼šè¯æ•°: {len(all_stats)}")
        for session_id, stats in all_stats.items():
            print(f"  {session_id}: {stats['total_messages']} æ¡æ¶ˆæ¯, {stats['estimated_tokens']} tokens")

        print(f"\nğŸ“‹ æ´»è·ƒ LLM ä¼šè¯: {LLMManager.list_active_conversations()}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    main()